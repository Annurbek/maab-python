# Task 1
from bs4 import BeautifulSoup

with open("weather.html", "r", encoding="utf-8") as file:
    soup = BeautifulSoup(file, 'html.parser')

max_temp = float('-inf')
max_day = ""
temperatures = []

print("üìÖ –ü—Ä–æ–≥–Ω–æ–∑ –ø–æ–≥–æ–¥—ã:\n")

for row in soup.select('tbody tr'):
    cols = [td.get_text(strip=True) for td in row.find_all('td')]
    if len(cols) != 3:
        continue

    day, temp_str, condition = cols
    try:
        temp = int(temp_str.replace('¬∞C', ''))
    except ValueError:
        continue

    temperatures.append(temp)
    print(f"{day}: {temp}¬∞C, {condition}")

    if temp > max_temp:
        max_temp = temp
        max_day = day

if temperatures:
    avg_temp = sum(temperatures) / len(temperatures)
    print(f"\nüå°Ô∏è –°–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {max_temp}¬∞C ({max_day})")
    print(f"üìä –°—Ä–µ–¥–Ω—è—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞: {avg_temp:.1f}¬∞C")
else:
    print("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –æ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–µ.")

# Task 2
import requests
from bs4 import BeautifulSoup
import sqlite3
import csv
from datetime import datetime

conn = sqlite3.connect('vacancys.db')
cursor = conn.cursor()

cursor.execute('''
    CREATE TABLE IF NOT EXISTS jobs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        title TEXT,
        company TEXT,
        location TEXT,
        description TEXT,
        link TEXT,
        updated_at TEXT,
        UNIQUE(title, company, location)
    );
''')
conn.commit()

url = "https://realpython.github.io/fake-jobs/"
response = requests.get(url)
soup = BeautifulSoup(response.text, "html.parser")
vacancys = soup.find_all("div", class_="card-content")

for vacancy in vacancys:
    title = vacancy.find("h2", class_="title is-5").get_text(strip=True)
    company = vacancy.find("h3", class_="subtitle is-6 company").get_text(strip=True)
    location = vacancy.find("p", class_="location").get_text(strip=True)

    links = vacancy.find_all("a", class_="card-footer-item")
    link = links[1].get("href") if len(links) >= 2 else None

    if link:
        try:
            job_response = requests.get(link)
            job_soup = BeautifulSoup(job_response.text, "html.parser")
            paragraph = job_soup.find("p", class_=False)
            description = paragraph.get_text(strip=True) if paragraph else "–ù–µ—Ç –æ–ø–∏—Å–∞–Ω–∏—è"
        except Exception:
            description = "–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–ø–∏—Å–∞–Ω–∏—è"
    else:
        description = "–ù–µ—Ç —Å—Å—ã–ª–∫–∏"

    updated_at = datetime.now().isoformat()

    cursor.execute('''
        SELECT description, link FROM jobs
        WHERE title = ? AND company = ? AND location = ?
    ''', (title, company, location))

    existing = cursor.fetchone()

    if existing:
        old_description, old_link = existing
        if old_description != description or old_link != link:
            cursor.execute('''
                UPDATE jobs
                SET description = ?, link = ?, updated_at = ?
                WHERE title = ? AND company = ? AND location = ?
            ''', (description, link, updated_at, title, company, location))
    else:
        cursor.execute('''
            INSERT INTO jobs (title, company, location, description, link, updated_at)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (title, company, location, description, link, updated_at))

    conn.commit()

def get_locations():
    cursor.execute("SELECT DISTINCT location FROM jobs")
    return [row[0] for row in cursor.fetchall()]

def get_companies():
    cursor.execute("SELECT DISTINCT company FROM jobs")
    return [row[0] for row in cursor.fetchall()]

def filter_by_location(location):
    cursor.execute("""
        SELECT title, company, location, description, link FROM jobs
        WHERE location = ?
    """, (location,))
    return cursor.fetchall()

def filter_by_company(company):
    cursor.execute("""
        SELECT title, company, location, description, link FROM jobs
        WHERE company = ?
    """, (company,))
    return cursor.fetchall()

def export_to_csv(filename="vacancies_export.csv"):
    cursor.execute("SELECT title, company, location, description, link FROM jobs")
    rows = cursor.fetchall()

    with open(filename, mode="w", encoding="utf-8", newline='') as file:
        writer = csv.writer(file)
        writer.writerow(["Title", "Company", "Location", "Description", "Link"])
        writer.writerows(rows)

    print(f"\n‚úÖ –î–∞–Ω–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –≤ {filename}")

if __name__ == "__main__":
    print("\nüîç –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –í–ê–ö–ê–ù–°–ò–ô üîç")
    find_arg = input("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø —Ñ–∏–ª—å—Ç—Ä–∞ (1: –ø–æ –ª–æ–∫–∞—Ü–∏–∏, 2: –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é –∫–æ–º–ø–∞–Ω–∏–∏, 3: —ç–∫—Å–ø–æ—Ä—Ç –≤ CSV): ")

    if find_arg == "1":
        locations = get_locations()
        print("–õ–æ–∫–∞—Ü–∏–∏:")
        for loc in locations:
            print("-", loc)
        argument = input("–í–≤–µ–¥–∏—Ç–µ –ª–æ–∫–∞—Ü–∏—é: ")
        results = filter_by_location(argument)

    elif find_arg == "2":
        companies = get_companies()
        print("–ö–æ–º–ø–∞–Ω–∏–∏:")
        for comp in companies:
            print("-", comp)
        argument = input("–í–≤–µ–¥–∏—Ç–µ –∫–æ–º–ø–∞–Ω–∏—é: ")
        results = filter_by_company(argument)

    elif find_arg == "3":
        export_to_csv()
        results = []

    else:
        print("–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä.")
        results = []

    if results:
        print("\nüéØ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n")
        for job in results:
            print(f"–ù–∞–∑–≤–∞–Ω–∏–µ: {job[0]}")
            print(f"–ö–æ–º–ø–∞–Ω–∏—è: {job[1]}")
            print(f"–õ–æ–∫–∞—Ü–∏—è: {job[2]}")
            print(f"–û–ø–∏—Å–∞–Ω–∏–µ: {job[3]}")
            print(f"–°—Å—ã–ª–∫–∞: {job[4]}")
            print("-" * 40)
    else:
        if find_arg != "3":
            print("–ù–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∞–∫–∞–Ω—Å–∏–π.")

#Task 3

import requests
import json

def get_products_by_category(category: str):
    url = "https://api.demoblaze.com/bycat"
    headers = {"Content-Type": "application/json"}
    payload = json.dumps({"cat": category})

    try:
        response = requests.post(url, headers=headers, data=payload, timeout=10)
        response.raise_for_status()
        return response.json().get("Items", [])
    except requests.RequestException as e:
        print(f"–û—à–∏–±–∫–∞ API: {e}")
        return []

all_data = {}
categories = ["phone", "notebook", "monitor"]

for category in categories:
    items = get_products_by_category(category)
    filtered = [{
        "title": p.get("title"),
        "price": p.get("price"),
        "description": p.get("desc"),
        "image": p.get("img"),
        "category": category
    } for p in items]
    all_data[category] = filtered
    print(f"‚úÖ {category}: {len(filtered)} —Ç–æ–≤–∞—Ä–æ–≤")

with open("products_all.json", "w", encoding="utf-8") as f:
    json.dump(all_data, f, ensure_ascii=False, indent=4)

print("\nüìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ products_all.json")